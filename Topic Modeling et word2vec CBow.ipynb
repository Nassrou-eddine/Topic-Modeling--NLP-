{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a27ff6-76a8-4645-8a7b-9231881fc822",
   "metadata": {},
   "source": [
    "## DVS et réduction de la dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c0efb6-05d8-4cd4-800e-5d78f2f7a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice réduite après Truncated SVD :\n",
      "[[ 1.54985297 -0.31297888]\n",
      " [ 1.54985297 -0.31297888]\n",
      " [ 0.87032429  1.11469082]]\n",
      "Valeurs singulières :\n",
      "[2.35829447 1.19935282]\n",
      "Composantes des termes (V) :\n",
      "[[ 0.43516215  0.43516215  0.55734541  0.55734541]\n",
      " [ 0.55734541  0.55734541 -0.43516215 -0.43516215]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Matrice document-terme\n",
    "A = np.array([\n",
    "    [1, 0, 1, 1],  # Document 1\n",
    "    [0, 1, 1, 1],  # Document 2\n",
    "    [1, 1, 0, 0]   # Document 3\n",
    "])\n",
    "\n",
    "# Appliquer Truncated SVD avec 2 dimensions latentes\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "A_reduit = svd.fit_transform(A)\n",
    "\n",
    "# Afficher la matrice réduite\n",
    "print(\"Matrice réduite après Truncated SVD :\")\n",
    "print(A_reduit)\n",
    "\n",
    "# Afficher les valeurs singulières\n",
    "print(\"Valeurs singulières :\")\n",
    "print(svd.singular_values_)\n",
    "\n",
    "# Composantes des termes dans l'espace latent\n",
    "print(\"Composantes des termes (V) :\")\n",
    "print(svd.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405110ae-7467-40c2-81ea-5babb9c49891",
   "metadata": {},
   "source": [
    "## Exemple de LSA avec des documents de texte naturel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66749ef5-c7c4-47fa-922a-f86138b605a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les mots dans le vocabulaire:\n",
      "['advertising' 'ai' 'algorithms' 'analysis' 'applications' 'artificial'\n",
      " 'attract' 'awareness' 'branch' 'brand' 'campaigns' 'crucial' 'customer'\n",
      " 'customers' 'data' 'digital' 'driven' 'effective' 'experiences' 'help'\n",
      " 'increase' 'intelligence' 'learning' 'machine' 'marketing' 'media'\n",
      " 'networks' 'neural' 'online' 'optimize' 'personalizes' 'sales' 'social'\n",
      " 'strategies' 'tools' 'used' 'uses']\n",
      "\n",
      "Projection des documents dans les deux premières espaces latents (LSA matrix):\n",
      "[[-0.      0.3474]\n",
      " [-0.      0.6823]\n",
      " [-0.      0.3293]\n",
      " [ 0.7116  0.    ]\n",
      " [ 0.6846  0.    ]\n",
      " [ 0.7116  0.    ]\n",
      " [-0.      0.3271]\n",
      " [-0.      0.7664]]\n",
      "\n",
      "Term contributions to each latent topic (components):\n",
      "(2, 37)\n",
      "   advertising     ai  algorithms  analysis  applications  artificial  \\\n",
      "0      -0.0000 -0.000     -0.0000   -0.0000        0.2057      0.4942   \n",
      "1       0.1086  0.328      0.1086    0.2424        0.0000      0.0000   \n",
      "\n",
      "   attract  awareness  branch   brand  ...  neural  online  optimize  \\\n",
      "0  -0.0000    -0.0000  0.2388 -0.0000  ...  0.2057 -0.0000   -0.0000   \n",
      "1   0.0989     0.0871  0.0000  0.0871  ...  0.0000  0.0871    0.1086   \n",
      "\n",
      "   personalizes   sales  social  strategies   tools    used    uses  \n",
      "0        0.2388 -0.0000 -0.0000     -0.0000 -0.0000  0.2057 -0.0000  \n",
      "1        0.0000  0.0871  0.0989      0.1086  0.0871  0.0000  0.0989  \n",
      "\n",
      "[2 rows x 37 columns]\n",
      "\n",
      "Topic 1: Dominant terms\n",
      "artificial: 0.4942\n",
      "intelligence: 0.4942\n",
      "customer: 0.2388\n",
      "experiences: 0.2388\n",
      "personalizes: 0.2388\n",
      "\n",
      "Topic 2: Dominant terms\n",
      "marketing: 0.4510\n",
      "campaigns: 0.4402\n",
      "ai: 0.3280\n",
      "driven: 0.2828\n",
      "effective: 0.2828\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Création des documents\n",
    "documents = [\n",
    "    \"Digital marketing uses social media to attract more customers.\",\n",
    "    \"Data analysis is crucial in marketing campaigns.\",\n",
    "    \"Online marketing tools help increase sales and brand awareness.\",\n",
    "    \"Artificial intelligence personalizes customer experiences.\",\n",
    "    \"Neural networks are used in artificial intelligence applications.\",\n",
    "    \"Machine learning is a branch of artificial intelligence.\",\n",
    "    \"AI algorithms optimize advertising strategies.\",\n",
    "    \"Marketing campaigns driven by AI are more effective.\"\n",
    "]\n",
    "\n",
    "# La vectorization TF-IDF avec suppression des mots vides\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# affichage des mots dans le vocabulaire\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "print(\"Les mots dans le vocabulaire:\")\n",
    "print(terms)\n",
    "\n",
    "# réduction de la dimension de la matrice TF-IDF\n",
    "lsa_model = TruncatedSVD(n_components=2)\n",
    "X_lsa = lsa_model.fit_transform(X)\n",
    "\n",
    "# Affichage de la matrice réduite, qui représente le lien de chaque document avec les topics\n",
    "print(\"\\nProjection des documents dans les deux premières espaces latents (LSA matrix):\")\n",
    "print(X_lsa.round(4))\n",
    "\n",
    "# Analyse des topics\n",
    "# lsa_model.components_ est la matrice Vk' dans le décomposition en valeurs singulière\n",
    "#cette matrice illsutre le lien de chaque terme dans le vocab avec les deux topics\n",
    "term_component_matrix = pd.DataFrame(lsa_model.components_, columns=terms).round(4)\n",
    "print(\"\\nTerm contributions to each latent topic (components):\")\n",
    "print(term_component_matrix.shape)\n",
    "print(term_component_matrix)\n",
    "# Affichage des mots les plus dominants dans chaque topic\n",
    "#boucle optionnelle juste pour identifier les topics \n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    terms_in_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_in_comp, key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nTopic {i+1}: Dominant terms\")\n",
    "    for term, weight in sorted_terms[:5]:  # Top 5 terms for each topic\n",
    "        print(f\"{term}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9d82c-7e69-403f-8eae-f53aee6c7ea2",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fc1094-cf39-4e0f-9a96-f0d907ac03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les mots dans le vocabulaire:\n",
      "['advertising' 'ai' 'algorithms' 'analysis' 'applications' 'artificial'\n",
      " 'attract' 'awareness' 'branch' 'brand' 'campaigns' 'crucial' 'customer'\n",
      " 'customers' 'data' 'digital' 'driven' 'effective' 'experiences' 'help'\n",
      " 'increase' 'intelligence' 'learning' 'machine' 'marketing' 'media'\n",
      " 'networks' 'neural' 'online' 'optimize' 'personalizes' 'sales' 'social'\n",
      " 'strategies' 'tools' 'used' 'uses']\n",
      "[[0.09927165 0.80398275 0.0967456 ]\n",
      " [0.78767734 0.10571562 0.10660704]\n",
      " [0.09171857 0.08953205 0.81874938]\n",
      " [0.10539824 0.78875692 0.10584484]\n",
      " [0.09903982 0.80143052 0.09952966]\n",
      " [0.10539824 0.78875692 0.10584484]\n",
      " [0.79011151 0.1045948  0.10529368]\n",
      " [0.78816103 0.10551444 0.10632453]]\n",
      "[[0.79190089 1.14687747 0.79190089 0.82429129 0.33460834 0.33458885\n",
      "  0.33478114 0.33417051 0.33471857 0.33417051 1.17404768 0.82429129\n",
      "  0.33471856 0.33478114 0.82429129 0.33478114 0.84325158 0.84325158\n",
      "  0.33471856 0.33417051 0.33417051 0.33458885 0.33471857 0.33471857\n",
      "  0.9880963  0.33478114 0.33460834 0.33460834 0.33417051 0.79190089\n",
      "  0.33471856 0.33417051 0.33478114 0.79190089 0.33417051 0.33460834\n",
      "  0.33478114]\n",
      " [0.33437129 0.33432761 0.33437129 0.33439107 0.77558872 1.37147818\n",
      "  0.72542225 0.3340059  0.82731916 0.3340059  0.33433577 0.33439107\n",
      "  0.82731924 0.72542225 0.33439107 0.72542225 0.33438146 0.33438146\n",
      "  0.82731924 0.3340059  0.3340059  1.37147818 0.82731916 0.82731916\n",
      "  0.56646365 0.72542225 0.77558872 0.77558872 0.3340059  0.33437129\n",
      "  0.82731924 0.3340059  0.72542225 0.33437129 0.3340059  0.77558872\n",
      "  0.72542225]\n",
      " [0.33487694 0.33481424 0.33487694 0.33491568 0.33497277 0.33494493\n",
      "  0.33501777 0.69937979 0.33511091 0.69937979 0.3348306  0.33491568\n",
      "  0.33511084 0.33501777 0.33491568 0.33501777 0.33489724 0.33489724\n",
      "  0.33511084 0.69937979 0.69937979 0.33494493 0.33511091 0.33511091\n",
      "  0.56706867 0.33501777 0.33497277 0.33497277 0.69937979 0.33487694\n",
      "  0.33511084 0.69937979 0.33501777 0.33487694 0.69937979 0.33497277\n",
      "  0.33501777]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Création des documents\n",
    "documents = [\n",
    "    \"Digital marketing uses social media to attract more customers.\",\n",
    "    \"Data analysis is crucial in marketing campaigns.\",\n",
    "    \"Online marketing tools help increase sales and brand awareness.\",\n",
    "    \"Artificial intelligence personalizes customer experiences.\",\n",
    "    \"Neural networks are used in artificial intelligence applications.\",\n",
    "    \"Machine learning is a branch of artificial intelligence.\",\n",
    "    \"AI algorithms optimize advertising strategies.\",\n",
    "    \"Marketing campaigns driven by AI are more effective.\"\n",
    "]\n",
    "\n",
    "# La vectorization TF-IDF avec suppression des mots vides\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Affichage des mots dans le vocabulaire\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "print(\"Les mots dans le vocabulaire:\")\n",
    "print(terms)\n",
    "\n",
    "# Application du modèle LDA\n",
    "lda = LatentDirichletAllocation(n_components=3, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "print(lda.transform(X)) #les distribution des topics selon les documents\n",
    "print(lda.components_) #les scores de chaque mots dans le vocab dans chaque sujet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01002280-60a7-44ae-9922-1dbff14ac81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sujets découverts par LDA :\n",
      "Sujet 1: ['campaigns', 'ai', 'marketing', 'effective', 'driven', 'data']\n",
      "Sujet 2: ['artificial', 'intelligence', 'experiences', 'customer', 'personalizes', 'branch']\n",
      "Sujet 3: ['online', 'sales', 'brand', 'help', 'increase', 'awareness']\n",
      "\n",
      "Distribution des topics pour chaque document (probabilités) :\n",
      "Document 1: [0.1 0.8 0.1]\n",
      "Document 2: [0.79 0.11 0.11]\n",
      "Document 3: [0.09 0.09 0.82]\n",
      "Document 4: [0.11 0.79 0.11]\n",
      "Document 5: [0.1 0.8 0.1]\n",
      "Document 6: [0.11 0.79 0.11]\n",
      "Document 7: [0.79 0.1  0.11]\n",
      "Document 8: [0.79 0.11 0.11]\n",
      "\n",
      "Log-vraisemblance du modèle (log-likelihood): -96.4895658942396\n",
      "\n",
      "Perplexité du modèle: 163.95092708893736\n",
      "\n",
      "Tous les mots pour chaque topic avec leurs poids :\n",
      "\n",
      "Topic 1:\n",
      "Mots principaux : ['campaigns', 'ai', 'marketing', 'effective', 'driven', 'data', 'analysis', 'crucial', 'advertising', 'strategies', 'algorithms']\n",
      "Poids : [1.17 1.15 0.99 0.84 0.84 0.82 0.82 0.82 0.79 0.79 0.79]\n",
      "\n",
      "Topic 2:\n",
      "Mots principaux : ['artificial', 'intelligence', 'experiences', 'customer', 'personalizes', 'branch', 'learning', 'machine', 'used', 'neural', 'networks']\n",
      "Poids : [1.37 1.37 0.83 0.83 0.83 0.83 0.83 0.83 0.78 0.78 0.78]\n",
      "\n",
      "Topic 3:\n",
      "Mots principaux : ['online', 'sales', 'brand', 'help', 'increase', 'awareness', 'tools', 'marketing', 'branch', 'machine', 'learning']\n",
      "Poids : [0.7  0.7  0.7  0.7  0.7  0.7  0.7  0.57 0.34 0.34 0.34]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Sortie 1 : Affichage des 3 sujets découverts par LDA, avec les 5 mots principaux de chaque sujet\n",
    "\n",
    "topics = lda.components_\n",
    "top_words_topic1 = [terms[i] for i in topics[0].argsort()[:-7:-1]] #[début, fin , step]= \n",
    "top_words_topic2 = [terms[i] for i in topics[1].argsort()[:-7 :-1]] #argsort() ordre ascendant\n",
    "top_words_topic3 = [terms[i] for i in topics[2].argsort()[:-7:-1]]\n",
    "\n",
    "print(\"\\nSujets découverts par LDA :\")\n",
    "print(f\"Sujet 1: {top_words_topic1}\")\n",
    "print(f\"Sujet 2: {top_words_topic2}\")\n",
    "print(f\"Sujet 3: {top_words_topic3}\")\n",
    "\n",
    "# Sortie 2 : Distribution des topics pour chaque document\n",
    "doc_topic_dist = lda.transform(X)\n",
    "print(\"\\nDistribution des topics pour chaque document (probabilités) :\")\n",
    "for doc_idx, topic_dist in enumerate(doc_topic_dist):\n",
    "    print(f\"Document {doc_idx + 1}: {np.round(topic_dist, 2)}\")\n",
    "\n",
    "# Sortie 3 : Log-likelihood (log-vraisemblance)\n",
    "log_likelihood = lda.score(X)\n",
    "print(f\"\\nLog-vraisemblance du modèle (log-likelihood): {log_likelihood}\")\n",
    "\n",
    "# Sortie 4 : Perplexité (mesure de qualité du modèle)\n",
    "perplexity = lda.perplexity(X)\n",
    "print(f\"\\nPerplexité du modèle: {perplexity}\")\n",
    "\n",
    "# Sortie 5 : Affichage complet des topics avec tous les mots et leurs poids\n",
    "print(\"\\nTous les mots pour chaque topic avec leurs poids :\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "    top_words = [terms[i] for i in topic.argsort()[:-11 - 1:-1]]  # Affichage des 10 mots principaux\n",
    "    print(f\"Mots principaux : {top_words}\")\n",
    "    print(f\"Poids : {np.round(topic[topic.argsort()[:-11 - 1:-1]], 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701731c4-ff38-4d86-8405-8afd300cd385",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae383e09-10b2-4d95-b726-5fd834379aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding du mot 'données':\n",
      "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Exemple minimal de corpus (liste de phrases)\n",
    "corpus = [\n",
    "    ['les', 'données', 'sont', 'importantes'],\n",
    "    ['les', 'données', 'sont', 'le', 'nouveau', 'pétrole'],\n",
    "    ['le', 'traitement', 'des', 'données', 'est', 'essentiel'],\n",
    "    ['la', 'science', 'des', 'données', 'transforme', 'le', 'monde']\n",
    "]\n",
    "\n",
    "# Créer et entraîner le modèle Word2Vec\n",
    "# min_count=1 permet de garder les mots qui apparaissent au moins une fois\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Exemple : obtenir l'embedding du mot \"données\"\n",
    "embedding = model.wv['données']\n",
    "print(f\"Embedding du mot 'données':\\n{embedding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4a280-3138-4c9a-b60b-0e70c6e491be",
   "metadata": {},
   "source": [
    "### Affichage de touts les embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc300fc-7cf2-4094-a4b3-d593710adc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice des Embeddings Word2Vec :\n",
      "                   0         1         2         3         4         5   \\\n",
      "données     -0.000536  0.000236  0.005103  0.009009 -0.009303 -0.007117   \n",
      "le          -0.008620  0.003666  0.005190  0.005742  0.007467 -0.006168   \n",
      "des          0.000095  0.003077 -0.006813 -0.001375  0.007669  0.007346   \n",
      "sont        -0.008243  0.009299 -0.000198 -0.001967  0.004604 -0.004095   \n",
      "les         -0.007139  0.001241 -0.007177 -0.002245  0.003719  0.005833   \n",
      "monde       -0.008727  0.002130 -0.000873 -0.009318 -0.009428 -0.001411   \n",
      "transforme   0.008132 -0.004457 -0.001068  0.001007 -0.000191  0.001148   \n",
      "science      0.008169 -0.004443  0.008987  0.008255 -0.004436  0.000303   \n",
      "la          -0.009579  0.008944  0.004164  0.009234  0.006644  0.002925   \n",
      "essentiel   -0.005156 -0.006668 -0.007777  0.008311 -0.001982 -0.006855   \n",
      "est          0.007089 -0.001568  0.007947 -0.009489 -0.008029 -0.006640   \n",
      "traitement   0.009770  0.008165  0.001281  0.005098  0.001408 -0.006455   \n",
      "pétrole     -0.001944 -0.005268  0.009447 -0.009299  0.004504  0.005404   \n",
      "nouveau     -0.009500  0.009562 -0.007771 -0.002646 -0.004906 -0.004967   \n",
      "importantes  0.007697  0.009121  0.001136 -0.008325  0.008425 -0.003696   \n",
      "\n",
      "                   6         7         8         9   ...        90        91  \\\n",
      "données      0.006459  0.008973 -0.005015 -0.003763  ...  0.001631  0.000190   \n",
      "le           0.001106  0.006047 -0.002840 -0.006174  ...  0.001088 -0.001576   \n",
      "des         -0.003673  0.002643 -0.008317  0.006205  ... -0.004509  0.005702   \n",
      "sont         0.002743  0.006940  0.006065 -0.007511  ... -0.007426 -0.001064   \n",
      "les          0.001198  0.002103 -0.004110  0.007225  ...  0.003137 -0.004713   \n",
      "monde        0.004433  0.003704 -0.006500 -0.006873  ...  0.009071  0.008938   \n",
      "transforme   0.006115 -0.000021 -0.003247 -0.001511  ... -0.002701  0.000444   \n",
      "science      0.004276 -0.003927 -0.005561 -0.006513  ...  0.002058 -0.004004   \n",
      "la           0.009804 -0.004424 -0.006803  0.004228  ... -0.005086  0.001131   \n",
      "essentiel   -0.004154  0.005144 -0.002869 -0.003750  ... -0.008977  0.008592   \n",
      "est         -0.004003  0.004989 -0.003814 -0.008320  ...  0.007512  0.001498   \n",
      "traitement  -0.001428  0.006449 -0.004617 -0.003993  ...  0.004774 -0.003262   \n",
      "pétrole     -0.001409  0.009007  0.009885 -0.005475  ...  0.002651 -0.002565   \n",
      "nouveau     -0.008024 -0.007784 -0.004553 -0.001275  ...  0.008380  0.007234   \n",
      "importantes  0.005742  0.004392  0.009690 -0.009293  ...  0.007096  0.001902   \n",
      "\n",
      "                   92        93        94        95        96        97  \\\n",
      "données      0.003474  0.000218  0.009619  0.005061 -0.008917 -0.007042   \n",
      "le           0.002197 -0.007882 -0.002717  0.002663  0.005347 -0.002392   \n",
      "des          0.009180 -0.004100  0.007965  0.005375  0.005879  0.000513   \n",
      "sont        -0.000795 -0.002563  0.009683 -0.000459  0.005874 -0.007448   \n",
      "les          0.005281 -0.004233  0.002642 -0.008046  0.006210  0.004819   \n",
      "monde       -0.008209 -0.003012  0.009887  0.005105 -0.001588 -0.008692   \n",
      "transforme  -0.003538 -0.000419 -0.000708  0.000823  0.008195 -0.005737   \n",
      "science     -0.008243  0.006279 -0.001949 -0.000666 -0.001771 -0.004536   \n",
      "la           0.002884 -0.001537  0.009932  0.008350  0.002416  0.007119   \n",
      "essentiel    0.004047  0.007470  0.009746 -0.007290 -0.009040  0.005836   \n",
      "est         -0.001265  0.005768 -0.005640  0.000039  0.009457 -0.005481   \n",
      "traitement  -0.009268  0.003787  0.007161 -0.005633 -0.007865 -0.002973   \n",
      "pétrole      0.006448 -0.007660  0.003394  0.000490  0.008732  0.005983   \n",
      "nouveau      0.001730 -0.001347 -0.005890 -0.004533  0.008648 -0.003135   \n",
      "importantes  0.005199  0.006381  0.001912 -0.006128 -0.000006  0.008268   \n",
      "\n",
      "                   98        99  \n",
      "données      0.000901  0.006393  \n",
      "le          -0.009510  0.004506  \n",
      "des          0.008213 -0.007019  \n",
      "sont        -0.002506 -0.005550  \n",
      "les          0.000787  0.003013  \n",
      "monde        0.002962 -0.006677  \n",
      "transforme  -0.001659  0.005571  \n",
      "science      0.004063 -0.004271  \n",
      "la           0.005891 -0.005580  \n",
      "essentiel    0.009391  0.003507  \n",
      "est          0.003814 -0.008113  \n",
      "traitement  -0.004932 -0.002315  \n",
      "pétrole      0.006815  0.007823  \n",
      "nouveau     -0.006339  0.009870  \n",
      "importantes -0.006099  0.009438  \n",
      "\n",
      "[15 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Obtenir les mots dans le vocabulaire\n",
    "\n",
    "words = list(model.wv.index_to_key)\n",
    "\n",
    "# Obtenir les embeddings pour tous les mots du vocabulaire\n",
    "embeddings_matrix = np.array([model.wv[word] for word in words])\n",
    "\n",
    "# Visualiser la matrice des embeddings sous forme de DataFrame pour voir les vecteurs associés à chaque mot\n",
    "embeddings_df = pd.DataFrame(embeddings_matrix, index=words)\n",
    "print(\"Matrice des Embeddings Word2Vec :\")\n",
    "print(embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943b558-2716-41b0-ac22-e875c616dfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
